* Posts
#+hugo_base_dir: ../
#+hugo_section: post
** DONE Scraping Customer Complaints - Better Business Bureau
:PROPERTIES:
:EXPORT_FILE_NAME: scraping-customer-complaints-better-business-bureau
:EXPORT_DATE: 2020-11-01
:EXPORT_HUGO_CUSTOM_FRONT_MATTER:  :tags ["Scraping" "Qualitative Data" "Content Analysis" "Methods"] :subtitle "The why and how behind scraping customer complaints from the Better Business Bureau" :featured true :categories ["Python"] :highlight true
:END:
*** Introduction

Customer complaints offer a unique insight into the sentiment around a product or service. From a company's perspective, scraping the reviews of their products sold on a third party platform may allow for a better understanding of the choices and preferences of their customers. Moreover, tracking customer complaints for competitors may also allow an organization to develop a competitive advantage with their own products and services.

From an academic perspective, customer complaints offer a window towards Electronic Logistics Service Quality (eLSQ) ([[https://onlinelibrary-wiley-com.proxy.lib.ohio-state.edu/doi/full/10.1111/j.2158-1592.2011.01014.x?casa_token=KCFnZ_oaccQAAAAA%3AxxzI2rZd9MEt5ZV9EN0NGUx6bLGpjFcKMuGL92FMqyxCilUoJRwBs4bApCrJynpTFuL3MmH70idNl90][Rao et al. 2011]]). Content analysis of the complaints can help not only in qualitative studies, but could also aid in operationalizing of constructs related to eLSQ and other customer-centric theories.

This post goes over a quick method to collect data from the Better Business Bureau. This post is not meant to be comprehensive, but merely serves as a proof of concept of how such data could be collected.

***  Scraping the Better Business Bureau:

For the purposes of this exercise, we will be using =Python 3.7= to scrape the data. We will make particular use of the =Selenium= libraries, and the =Firefox webdriver= to emulate the behavior of an actual user browsing the website.

Note: You can find tutorials about how to use [[https://selenium-python.readthedocs.io/][selenium here]]. You will need to make sure you install the correct version of the webdriver based on the version of firefox you are currently using.


**** Importing the libraries

#+BEGIN_SRC python
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait                 # to wait for an element to become available on the page
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.touch_actions import TouchActions        # to allow for clicking of buttons
from selenium.webdriver.common.action_chains import ActionChains        # to allow for clicking of buttons, and linking it to other behavior
import time

#+END_SRC

We import the selenium and webdriver libraries, and especially the =TouchActions= and =ActionChains=, as it allows for scrolling the page, and clicking on the =next/more= buttons on the website.

The next step is to initialize the webdriver, and fetch a webpage. In this case, we are going to get the complaints from Amazon. This is especially interesting as Amazon employs workers to address the issues raised on BBB.
**** Loading the webpage

#+BEGIN_SRC python
driver=webdriver.Firefox()
source=0
driver.get("https://www.bbb.org/us/wa/seattle/profile/ecommerce/amazoncom-1296-7039385/complaints")
time.sleep(5)

#+END_SRC

The above code will open the complaints page for Amazon's profile on the BBB. the =time.sleep(5)= makes the script wait for 5 seconds before continuing. This ensures that the webpage is fetched and loaded before we continue.
**** Gathering the data

There are multiple ways to get the data from this page. I am particularly interested in =Problems with the Product or Service= category of complaints. Although there exist multiple ways to navigate to this page using python, the easy way for me to scrape this category is simply click on the 11,314 (which is the number of current issues on BBB in this category) and then scrape that.

After I click on the =11,314= (as of Nov 1, 2020), the new page loads and shows me about 20 complaints at a time. At the bottom of this page is a =More= button, which upon clicking loads an additional 20 complaints, with the =More= button available again. So to load all the complaints we can iteratively scroll to the bottom of the page, click the button, and scroll to the bottom again till no more complaints are available.

After all the complaints are available, we can take all the text present on the page and dump it into a text file for further processing.
The code block below scrapes all the complaints and puts them in a text file. It also handles =TimeoutError= in case one arises.

#+BEGIN_SRC python
try:
    link=WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.LINK_TEXT,"11,314")))  # text for "Problem with a Product or Service"
    link.click()
    time.sleep(10)
    count=0
    while (count<400):
        try:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            #find the more button
            more_button = WebDriverWait(driver,10).until(EC.visibility_of_element_located((By.XPATH,"/html/body/div[1]/div/div/div/main/div/div[5]/div/div[3]/button")))
            ActionChains(driver).move_to_element(more_button).click().perform()
            time.sleep(6)
            count=count+1
            print(count)
        except TimeoutError:
            break
    content= driver.find_element_by_tag_name('body')
    foo=content.text
    text_file=open("/path/to/file.txt","w")
    text_file.write(foo)
finally:
    print("Done!")
    time.sleep(3)
    driver.quit()
print("Finished!")

#+END_SRC


**** Next Steps

After the data has been written to a text file, we can do various types of text analytics on it. These might include frequency counts, sentiment analysis or topic modeling. But that is for a future blog post.
** DONE Halloween Costumes - a spooky story
:PROPERTIES:
:EXPORT_FILE_NAME: halloween-imports
:EXPORT_DATE: <2020-11-02 Mon 12:14>
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :tags ["SCM", "Imports", "Freight"] :subtitle "Seaborne imports of Halloween costumes and outfits decline" :featured true :categories ["SCM" ] :highlight false 
:END:

A recent report from [[https://panjiva.com/][Panjiva]] states that U.S. seaborne imports of Halloween costumes and outfits declined over 7.2% over the four month period starting June 1. Now this could be generally attributed to a slowdown in the economy, especially for non-essential products; or to social distancing because of the coronavirus pandemic which was expected to lead to fewer Halloween parties.

Another reason could be the tariffs stemming from the US-China trade war. A large number of the Halloween related manufacturing companies have operations in China. Moreover, according to the Halloween and Costume Association (which represents 50 of such companies),[[https://www.nbcnews.com/business/business-news/vampire-makeup-pet-costumes-halloween-set-take-hit-trump-s-n1068661][manufacturers are expected to absorb these tariffs as retailers are not keen to raise prices]]. These sentiments are expected to be stronger in the face of a pandemic. All these factors taken together, perhaps have led to such a marked drop in costume import.

Interestingly, the hardest hit categories were zombie costumes (-42% YoY) and vampire costumes (-20.2% YoY).  Medical themed outfits also suffered a decline as a category - perhaps of the association with the pandemic.

Another reason could be the lack of Movie blockbusters, as they form insipiration for each years new costumes. Given the release delays for most movies, and movie theatre chains [[https://www.npr.org/sections/coronavirus-live-updates/2020/10/05/920367787/regal-movie-chain-will-close-all-536-u-s-theaters-on-thursday][closing US operations]], the last blockbuster in the US was Toy Story 4 from 2019 - which in turn spurred costume sales last year.
** DONE Reddit Comment Analysis
:PROPERTIES:
:EXPORT_FILE_NAME: reddit-comment-analysis
:EXPORT_DATE: <2020-11-14 Sat 14:28>
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :tags ["Qualitative Data" "Content Analysis" "Methods"] :subtitle "What can Reddit Comments Tell us" :featured true :categories ["Python"] :highlight true 
:END:


We have already scraped and stored the reddit comments from /r/CouriersofReddit, /r/PostMates, and /r/DoorDash. The data is now in a pickle file, and is of the following format:

- PK: Primary Key - the reddit post ID
- Title: Title of the post.
- URL: link to the post.
- Time: time of creation of the post.
- Comments: the particular comment.
- Comment_Time: The time the comment was created.
- Is_Root_Comment: Identifies if the comment is a root comment in the post submission.
- Comment_Parent: What which comment is the parent of this comment, has the comment ID in it.
- Comment_PK: the comment ID. 



Import all the libraries that are needed - and read in the pickle into a data frame. 

#+begin_src python :results output :session comment-analysis
import pandas as pd
import numpy as np
import matplotlib
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt

df = pd.read_pickle("~/Dropbox/School/My Papers in Progress/Crowdsourced Delivery/flat-comment-df.pkl")
df=df.drop_duplicates()
#+end_src

#+RESULTS:

We might need to save the file as a TSV for sharing with others. The TSV preserves the commas in the comments, and can also be ingested by Excel, R, and Tableau.

#+begin_src python :results output  :session comment-analysis

#writefile = df.to_csv('~/Dropbox/School/My Papers in Progress/Crowdsourced Delivery/flat-comment.tsv', sep='\t')

#+end_src


*** Searching the corpus for keywords
Ideally, we want to be able to create a smaller dataset for analysis from this big data set. We create an array with all the search terms we want to search for, and then assign the search result into another data frame. We can then print out how many posts and comments are present in the resulting dataset.

#+begin_src python :results output :session comment-analysis :exports both

search_terms = ['race', 'racism', 'racist', 'discriminate']
searchResult = df[df['Comments'].str.contains('|'.join(search_terms))] #creates the result dataframe.
print("The number of comments with search terms:", len(searchResult.index))
print("Total number of Posts", len(pd.unique(searchResult['PK'])))

#+end_src
#+RESULTS:
: The number of comments with search terms: 1147
: Total number of Posts 583

**** Top 10 posts in the dataset
After we have created the dataset, it is evident from the last step that there are 1147 comments in 583 posts. This indicates that there might be some posts with very few comments, and some with many. Let's get a list of the top 10 posts with the most comments.

#+begin_src python :session comment-analysis :results output :exports both

topPosts = searchResult['Title'].value_counts()[:10].index.tolist()
for post in topPosts:
    print(post)

#+end_src

#+RESULTS:
#+begin_example
I feel like this promotes racism rather than helps..
Deactivated Because a Racist manager not only called the police on me, but also reported me to Doordash
Postmates driver encounters deranged woman
The MPLS cop that killed George Floyd can't get doordash delivered. 
I hit my 1000th delivery today. A few thoughts about my experiences.
Never thought I'd get one of these...
When the $3 orders can no longer keep you afloat
This is why your food is cold
Courier (rightfully) fears for his life
So I was delivering the other day...
#+end_example

**** Save the dataset
We might want to save this dataset as a pickle and a TSV if needed. Lets do that.
#+begin_src python :session comment-analysis :results output 

writefile = searchResult.to_csv('~/Dropbox/School/My Papers in Progress/Crowdsourced Delivery/search-result-flat-comment.tsv', sep='\t')
## Write the file to pickle for other scripts to use.
searchResult.to_pickle("~/Dropbox/School/My Papers in Progress/Crowdsourced Delivery/search-result-flat-comment.pkl")

#+end_src

#+RESULTS:

*** Word Cloud:
I want to evaluate quickly what kind of themes are in the present in the data set. A word cloud usually works pretty well. I think because we are using the terms "race/ism/ist" in the search,  I would probably want to remove those words along with other stop words.

First thing we do is to put all the text in the comments into one variable, and look at how many words there are in total.


#+begin_src python :session comment-analysis :results output 

text="".join(comment for comment in searchResult.Comments)
print ("There are {} words in the combination of all comments.".format(len(text)))
stopwords = set(STOPWORDS)
stopwords.update(["racist", "racism", "race", "people", "will","still"])
# Generate a word cloud image
wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)
#+end_src

#+RESULTS:
: There are 498150 words in the combination of all comments.
Show the wordcloud, and save it to a file


#+begin_src python :session comment-analysis :results output

plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

wordcloud.to_file("~/Dropbox/School/My Papers in Progress/Crowdsourced Delivery/wordcloud-search-comments.png")

#+end_src

#+RESULTS:
*** Sentiment Analysis
Although there are multiple types of sentiment analysis engines out there, I do not want to use a custom training paradigm, or me having to train the model myself (trying to save some time and computer cycles). Moreover, there have been recent developments with sentiment analysis that I want to leverage. Enter VADER.
**** VADER
VADER stands for Valence Aware Dictionary sEntiment Reasoner, and is a lexicon and rule-based sentiment analysis tool that is *specifically attuned to sentiments expressed in social media*. This makes it particularly useful for analysing reddit comments. VADER uses a combination of a sentiment lexicon - which is a list of lexical features (words) which are generally labelled according to their semantic orientation as either positive or negative, and also the polarity of the positive and negative sentiment.

VADER was developed using Amazon's Mechanical Turk platform to get most of their ratings, and can be considered the gold standard of sentiment lexicons.
**** Advantages of VADER
- works exceedingly well for social media text, yet readily generalizes to other domains. 
- doesn't require any training data - its created using generalizable, valence based, human curated sentiment lexicon
- fast : you can even use this with streaming data
- It does not suffer from a speed-performance tradeoff
**** Implementing VADER

You can download vader via pip, using the following command
=pip3 install vaderSentiment=

We then want to create a new dataframe, I could do it in existing dataframe, but I am trying to not get it corrupted. Create the new DF with these additional columns =Comment_Neg,Comment_Neu, Comment_Pos, Comment_Comp=.

We can then take the sentiment of each comment in the dataframe, and add all the information back to the new data frame. 
#+begin_src python :results output :session comment-analysis :exports both
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

column_names = ["PK","Title", "URL", "Time", "Comment_PK", "Comments", "Comment_Time", "Comment_Score", "Is_Root_Comment","Comment_Parent", "Comment_Neg","Comment_Neu", "Comment_Pos", "Comment_Comp"]

postSentimentDF=pd.DataFrame(columns=column_names)

sia_obj = SentimentIntensityAnalyzer()  #create a SentimentIntensityAnalyzer object
for index,comment in searchResult.iterrows():
    sentiment_dictionary = sia_obj.polarity_scores(comment["Comments"])
    postSentimentDF = postSentimentDF.append({'PK':comment['PK'], 'Title':comment['Title'], 'URL':comment['URL'], 'Time':comment['Time'], "Comments":comment['Comments'], "Comment_Time":comment['Comment_Time'], "Comment_Score":comment['Comment_Score'], "Is_Root_Comment":comment['Is_Root_Comment'], "Comment_Parent":comment['Comment_Parent'], 'Comment_PK':comment['Comment_PK'], 'Comment_Neg':sentiment_dictionary['neg'], 'Comment_Neu':sentiment_dictionary['neu'], 'Comment_Pos':sentiment_dictionary['pos'], 'Comment_Comp':sentiment_dictionary['compound']},ignore_index=True )
    #print(comment)

print(postSentimentDF.head())
#+end_src

#+RESULTS:
:        PK                                              Title                     URL                 Time Comment_PK  ... Comment_Parent Comment_Neg Comment_Neu Comment_Pos Comment_Comp
: 0  fshgt1                                                Lol  https://redd.it/fshgt1  2020-03-31 17:45:09    fm3gjaj  ...      t3_fshgt1       0.298       0.702       0.000      -0.9163
: 1  gbzdoc  Well everyone. My first controversial delivery...  https://redd.it/gbzdoc  2020-05-02 03:14:19    fp8kqba  ...     t1_fp8dz0u       0.100       0.814       0.086      -0.0772
: 2  gbzdoc  Well everyone. My first controversial delivery...  https://redd.it/gbzdoc  2020-05-02 03:14:19    fp968s1  ...     t1_fp8p0fs       0.000       0.860       0.140       0.8923
: 3  hjgrtz                        I was hacked, $338 was lost  https://redd.it/hjgrtz  2020-07-01 19:07:32    fwme4y4  ...      t3_hjgrtz       0.025       0.930       0.045       0.2732
: 4  hjgrtz                        I was hacked, $338 was lost  https://redd.it/hjgrtz  2020-07-01 19:07:32    fwmg9a5  ...     t1_fwme4y4       0.038       0.962       0.000      -0.2960
: 
: [5 rows x 14 columns]

Let us go ahead and save this file in pickle and TSV format.

#+begin_src python :results output :session comment-analysis
writefile = postSentimentDF.to_csv('~/Dropbox/School/My Papers in Progress/Crowdsourced Delivery/sentiment-search-result-flat-comment.tsv', sep='\t')
## Write the file to pickle for other scripts to use.
postSentimentDF.to_pickle("~/Dropbox/School/My Papers in Progress/Crowdsourced Delivery/sentiment-search-result-flat-comment.pkl")

#+end_src


#+RESULTS:
*** Topic Modeling of Posts/Comments

*** Data Cleaning
Before we actually get started with topic modelling, we have to do a bit of data cleaning.

Let's start by importing the full dataset into a data frame for Topic Modeling.

#+begin_src python :results output :session TM
#Data Manipulation and Storage
import pandas as pd

# Text Cleaning
import string
import re
import nltk 
from nltk.corpus import stopwords 
from nltk.stem import WordNetLemmatizer

from tqdm import tqdm



df = pd.read_pickle("~/Dropbox/School/My Papers in Progress/Crowdsourced Delivery/flat-comment-df.pkl")
df=df.drop_duplicates()
#+end_src

#+RESULTS:

Initiate the stopwords and wordnet lemmatizers. After that we iterate through the "Comments" convert all words to lower case, remove any links that might be present. After that we output the cleaned, tokenized and lemmitized comments into another data frame.

#+begin_src python :sesstion TM
wordnet_lemmatizer = WordNetLemmatizer()
stopwords_english = stopwords.words('english')

for index, comment in tqdm(df.iterrows()):
    text = comment['Comments']
    #make string lowercase
    text = text.lower()
    # remove links
    text = re.sub(r'^https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)

    #tokenize
    tokens = nltk.word_tokenize(text)

    #clean text 
    clean_text = []
    for word in tokens:
        if (word not in stopwords_english and word not in string.punctuation): 
            token = wordnet_lemmatizer.lemmatize(word)
            clean_text.append(token)
            
    #remove words of length 3 or smaller        
    clean_text = [token for token in clean_text if len(token) > 2] 
    #clean_text = " ".join(clean_text)
    comment["Comments"] =clean_text
TMdf= df.groupby(['PK', 'Title', 'URL'])['Comments'].apply(list).reset_index(name='total_comments_clean')

#+end_src
#+RESULTS:

Finally, we save the df to a pickle so that we do not have to run through this whole process again.

#+begin_src python :results output :session TM
TMdf.to_pickle("~/Dropbox/School/My Papers in Progress/Crowdsourced Delivery/lemmatized-flat-comment-df.pkl")
#+end_src
** TODO Academic Note Taking
:PROPERTIES:
:EXPORT_FILE_NAME: academic-note-taking
:EXPORT_DATE: <2021-02-17 Wed 20:17>
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :tags something :subtitle booyea :featured false :categories abc :highlight true
:END:
